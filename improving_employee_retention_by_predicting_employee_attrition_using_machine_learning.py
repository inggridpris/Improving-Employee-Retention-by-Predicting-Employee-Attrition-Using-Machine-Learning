# -*- coding: utf-8 -*-
"""Improving Employee Retention by Predicting Employee Attrition Using Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GNWyBeWYARb1v2LNnBoh-zWwM-LqJAt
"""

pip install --upgrade pandas

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

print('Numpy Version:', np.__version__)
print('Pandas Version:', pd.__version__)
print('Seaborn Version:', sns.__version__)

from matplotlib import rcParams
rcParams['figure.figsize'] = 12, 4
rcParams['lines.linewidth'] = 3
rcParams['xtick.labelsize'] = 'x-large'
rcParams['ytick.labelsize'] = 'x-large'

df= pd.read_excel('/content/drive/My Drive/Improving Employee Retention by Predicting Employee Attrition Using Machine Learning/Improving Employee Retention by Predicting Employee Attrition Using Machine Learning.xlsx')

df = pd.read_excel('Improving Employee Retention by Predicting Employee Attrition Using Machine Learning.xlsx')

df

df.info()

df.sample(5)

df.isna().sum()

df.info()

nums=['SkorKepuasanPegawai','JumlahKeikutsertaanProjek','JumlahKeterlambatanSebulanTerakhir','JumlahKetidakhadiran','IkutProgramLOP']
cats=['AlasanResign']

df[nums].describe()

df[cats].describe()

df['AlasanResign'].unique()

df.duplicated().sum()

df['PernahBekerja'].unique()

df['PernahBekerja'].sample(10)

df['StatusKepegawaian'].unique()

df1=df.copy()

df1['PernahBekerja']=df1['StatusKepegawaian'].apply(lambda x: 'No' if x=='Internship' else 'Yes')

df1

df1.sample (5)

df1['PernahBekerja'].unique()

df1.isna().sum()

df1['SkorKepuasanPegawai']=df1['SkorKepuasanPegawai'].fillna(df1['SkorKepuasanPegawai'].mean())
df1['JumlahKeikutsertaanProjek']=df1['JumlahKeikutsertaanProjek'].fillna(df1['JumlahKeikutsertaanProjek'].mean())
df1['JumlahKeterlambatanSebulanTerakhir']=df1['JumlahKeterlambatanSebulanTerakhir'].fillna(df1['JumlahKeterlambatanSebulanTerakhir'].mean())
df1['JumlahKetidakhadiran']=df1['JumlahKetidakhadiran'].fillna(df1['JumlahKetidakhadiran'].mean())
df1['AlasanResign']=df1['AlasanResign'].fillna('masih_bekerja')

df1.isna().sum()

df2 = df1.drop(columns=['IkutProgramLOP'])

df2.info()

df2

df2['TanggalHiring']=df2['TanggalHiring'].dt.strftime('%y/%m/%d')
df2['TanggalPenilaianKaryawan']=df2['TanggalPenilaianKaryawan'].dt.strftime('%y/%m/%d')

df2.info()

df2['TanggalHiring']=pd.to_datetime(df2['TanggalHiring'])
df2['TanggalPenilaianKaryawan']=pd.to_datetime(df2['TanggalPenilaianKaryawan'])

df2.info()

df2['TanggalResign'] = pd.to_datetime(df2['TanggalResign'], errors='coerce')

df2.info()

df2

df2['year_resign']=df2['TanggalResign'].dt.year

df2['year_hiring']=df2['TanggalHiring'].dt.year

df2

df2['year_hiring'].unique()

jumlah_hiring=df2.groupby('year_hiring').agg({'Username':'count'}).reset_index()
jumlah_hiring.columns=['year','total_karyawan']

jumlah_hiring



jumlah_resign=df2.groupby('year_resign').agg({'Username':'count'}).reset_index()
jumlah_resign.columns=['year','jumlah_karyawan']

jumlah_resign

total_seluruh=jumlah_hiring.merge(jumlah_resign,on='year', how='outer')

total_seluruh

total_seluruh['jumlah_karyawan']=total_seluruh['jumlah_karyawan'].fillna(0)
total_seluruh['total_karyawan']=total_seluruh['total_karyawan'].fillna(0)

total_seluruh

totalresign_tahun=total_seluruh.groupby('year').agg({'jumlah_karyawan':'sum'}).reset_index()
totalresign_tahun.columns=['year','totalresign']

totalresign_tahun

totalhiring_tahun=total_seluruh.groupby('year').agg({'total_karyawan':'sum'}).reset_index()
totalhiring_tahun.columns=['year','totalhiring']

totalhiring_tahun

total_seluruh['perubahan']=total_seluruh['total_karyawan']-total_seluruh['jumlah_karyawan']

total_seluruh

total_seluruh = total_seluruh[list(total_seluruh.columns[~total_seluruh.columns.duplicated()])]

total_seluruh.info()

import plotly.graph_objects as go

pip install waterfallcharts

import waterfall_chart
waterfall_chart.plot(total_seluruh['year'],total_seluruh['perubahan'])

sns.barplot(y='year', x='jumlah_karyawan', data=total_seluruh, orient='h')
plt.title('Jumlah Karyawan Resign setiap Tahunnya', fontsize='14', color='Black')
plt.xlabel('Jumlah Karyawan Resign', fontsize='12', color='Brown')
plt.ylabel('Year', fontsize='12', color='Brown')
plt.tight_layout()

sns.barplot(y='year', x='total_karyawan', data=total_seluruh, orient='h')
plt.title('Jumlah Karyawan Masuk setiap Tahunnya', fontsize='14', color='Brown')
plt.xlabel('Jumlah Karyawan Masuk', fontsize='12', color='Black')
plt.ylabel('Year', fontsize='12', color='Black')

df3=df2.copy()

df3

df3.info()

df4=df3.groupby(['Pekerjaan','AlasanResign']).agg({'Username':'count'}).reset_index()
df4.columns=['Pekerjaan','AlasanResign','jumlahkaryawan']
belum_resign=df4[df4['AlasanResign']=='masih_bekerja']

belum_resign

df5=df3.groupby(['Pekerjaan','AlasanResign']).agg({'Username':'count'}).reset_index()
df5.columns=['Pekerjaan','Alasan_Resign','jmlhkaryawan']
sudah_resign=df5[df5['Alasan_Resign']!='masih_bekerja']

sudah_resign

employee=belum_resign.merge(sudah_resign, on='Pekerjaan', how='outer')

employee

employee['jmlhkaryawan']=employee['jmlhkaryawan'].fillna(0)

employee['totalkarya']=employee['jumlahkaryawan']+employee['jmlhkaryawan']

employee

employee['persenkarya']=employee['jumlahkaryawan']/employee['totalkarya']

employee

import plotly.express as px

fig= px.sunburst(employee, path=['Pekerjaan'], values='persenkarya', title='Persentase karyawan yang masih bekerja', width=750, height=750)
fig.show()

fig= px.sunburst(employee, path=['Pekerjaan','AlasanResign'], values='persenkarya', title='Employee who still stay', width=750, height=750)
fig.show()

fig= px.sunburst(employee, path=['Pekerjaan','Alasan_Resign'], values='persenkarya', title='Resign Reason for employee', width=750, height=750)
fig.show()

fig= px.sunburst(employee, path=['Pekerjaan','AlasanResign','Alasan_Resign'], values='persenkarya', title='Resign Reason for employee', width=750, height=750)
fig.show()

sns.barplot(x='persenkarya', y='Pekerjaan', data=employee, color='Salmon')
plt.title('Presentasi karyawan yang tidak resign sesuai dengan pekerjaanya', fontsize='14', color='Blue')
plt.xlabel('Presentase', fontsize='12', color='Green')
plt.ylabel('Bidang Pekerjaan', fontsize='12', color='Green')
plt.tight_layout()

df3.info()

df6=df3.groupby(['Pekerjaan','AlasanResign','JenjangKarir','PerformancePegawai']).agg({'EnterpriseID':'count'}).reset_index()
df6.columns=['Pekerjaan','Alasan_Resign','Jenjang_karir','Performance_pegawai','jmlhkaryawan']
data_analyst=df6[df6['Pekerjaan']=='Data Analyst']

data_analyst

fig= px.sunburst(data_analyst, path=['Pekerjaan','Alasan_Resign','Jenjang_karir','Performance_pegawai'], values='jmlhkaryawan', title='Data Analyst Department', width=750, height=750)
fig.show()

df7=df3.copy()

df7.info()

df7.duplicated().sum()

df7.isna().sum()

df7['year_resign']=df7['year_resign'].fillna(0)
df7['TanggalResign']=df7['TanggalResign'].fillna(0)

df7.isna().sum()

df7.info()

nums = ['SkorSurveyEngagement','SkorKepuasanPegawai','JumlahKeikutsertaanProjek','JumlahKeterlambatanSebulanTerakhir','JumlahKetidakhadiran']
cats = ['StatusPernikahan','JenisKelamin','StatusKepegawaian','Pekerjaan','JenjangKarir','PerformancePegawai','AsalDaerah','HiringPlatform','TingkatPendidikan','PernahBekerja','AlasanResign']

df7[nums].describe()

df7[cats].describe()

for col in cats:
   print(f'''Value count kolom {col}:''')
   print(df7[col].value_counts())
   print()

features = nums
plt.figure(figsize=(8, 10))
for i in range(0, len(features)):
    plt.subplot(4, 5, i+1)
    sns.boxplot(y=df7[features[i]], color='green', orient='v')
    plt.tight_layout()

features = nums
plt.figure(figsize=(12, 10))
for i in range(0, len(nums)):
    plt.subplot(2, 3, i+1)
    sns.kdeplot(x=df7[features[i]], color='green')
    plt.xlabel(features[i])
    plt.tight_layout()

plt.figure(figsize=(12, 10))
for i in range(0, len(cats)):
    plt.subplot(6, 3, i+1)
    sns.countplot(x = df7[cats[i]], color='green', orient='v')
    plt.tight_layout()

df7.corr()

plt.figure(figsize=(10, 10))
sns.heatmap(df7.corr(), cmap='Blues', annot=True, fmt='.2f')

plt.figure(figsize=(20, 20))
sns.pairplot(df7[nums], diag_kind='kde')

df7.info()

df7['Resign']=df7['AlasanResign'].apply(lambda x: 'No' if x=='masih_bekerja' else 'Yes')

df7.info()

nums = ['SkorSurveyEngagement','SkorKepuasanPegawai','JumlahKeikutsertaanProjek','JumlahKeterlambatanSebulanTerakhir','JumlahKetidakhadiran']

df7['log_SkorSurveyEngagement']= np.log(df7['SkorSurveyEngagement']+np.finfo(float).eps)
df7['log_SkorKepuasanPegawai']= np.log(df7['SkorKepuasanPegawai']+np.finfo(float).eps)
df7['log_JumlahKeikutsertaanProjek']= np.log(df7['JumlahKeikutsertaanProjek']+np.finfo(float).eps)
df7['log_JumlahKeterlambatanSebulanTerakhir']= np.log(df7['JumlahKeterlambatanSebulanTerakhir']+np.finfo(float).eps)
df7['log_JumlahKetidakhadiran']= np.log(df7['JumlahKetidakhadiran']+np.finfo(float).eps)

log=['log_SkorSurveyEngagement','log_SkorKepuasanPegawai','log_JumlahKeikutsertaanProjek','log_JumlahKeterlambatanSebulanTerakhir','log_JumlahKetidakhadiran']

features = log
plt.figure(figsize=(12, 10))
for i in range(0, len(log)):
    plt.subplot(5, 5, i+1)
    sns.kdeplot(x=df7[features[i]], color='green')
    plt.xlabel(features[i])
    plt.tight_layout()

df7=df7.drop(columns='JumlahKetidakhadiran')

df7=df7.drop(columns=['log_SkorSurveyEngagement','log_SkorKepuasanPegawai','log_JumlahKeikutsertaanProjek','log_JumlahKeterlambatanSebulanTerakhir'])

df7.info()

nums1 = ['SkorSurveyEngagement','SkorKepuasanPegawai','JumlahKeikutsertaanProjek','JumlahKeterlambatanSebulanTerakhir','log_JumlahKetidakhadiran']


from scipy import stats
print(f'Jumlah baris sebelum memfilter outlier: {len(df7)}')

filtered_entries = np.array([True] * len(df7))

for col in nums1:
    zscore = abs(stats.zscore(df7[col])) # hitung absolute z-scorenya
    filtered_entries = (zscore < 3) & filtered_entries # keep yang kurang dari 3 absolute z-scorenya
    
df8 = df7[filtered_entries] # filter, cuma ambil yang z-scorenya dibawah 3

print(f'Jumlah baris setelah memfilter outlier: {len(df8)}')

df8.info()

df8['StatusPernikahan'].replace('-', 'Lainnya', inplace=True)

df8['HiringPlatform'].replace('Google_Search', 'Website', inplace=True)

df8['HiringPlatform'].replace('On-line_Web_application', 'Website', inplace=True)

df8['HiringPlatform'].replace('Other', 'Website', inplace=True)

cats_one = ['StatusPernikahan','Pekerjaan','AsalDaerah','HiringPlatform','AlasanResign']
for cats_one in ['StatusPernikahan','Pekerjaan','AsalDaerah','HiringPlatform','AlasanResign']:
    onehots = pd.get_dummies(df8[cats_one], prefix=cats_one)
    df8 = df8.join(onehots)

df8['PernahBekerja'].unique()

mapping_jk={'Wanita':0, 'Pria':1}
df8['JenisKelamin']=df8['JenisKelamin'].map(mapping_jk)

mapping_sp={'FullTime':0, 'Outsource':1, 'Internship':2}
df8['StatusKepegawaian']=df8['StatusKepegawaian'].map(mapping_sp)

mapping_jka={'Senior_level':0, 'Mid_level':1, 'Freshgraduate_program':2}
df8['JenjangKarir']=df8['JenjangKarir'].map(mapping_jka)

mapping_pp={'Sangat_bagus':0, 'Bagus':1, 'Biasa':2, 'Kurang':3, 'Sangat_kurang':4}
df8['PerformancePegawai']=df8['PerformancePegawai'].map(mapping_pp)

mapping_pendidikan={'Doktor':0, 'Magister':1, 'Sarjana':2}
df8['TingkatPendidikan']=df8['TingkatPendidikan'].map(mapping_pendidikan)

mapping_resign={'No':0, 'Yes':1}
df8['Resign']=df8['Resign'].map(mapping_resign)

mapping_pb={'Yes':0, 'No':1}
df8['PernahBekerja']=df8['PernahBekerja'].map(mapping_pb)

df8.info()

df8=df8.drop(columns=['StatusPernikahan','Pekerjaan','AsalDaerah','HiringPlatform','AlasanResign'])

df8.info()

df9=df8.drop(columns=['Username','EnterpriseID','NomorHP','Email','TanggalLahir','TanggalHiring','TanggalPenilaianKaryawan','TanggalResign','year_resign'])

df9.info()

corrmat = df9.corr() #menghitung korelasi dengan fungsi corr()
top_corr_features = corrmat.index #menampilkan index
plt.figure(figsize=(25,25)) #membuat plot dengan ukuran 20x20
g=sns.heatmap(df9[top_corr_features].corr(),annot=True,cmap="RdYlGn")

a = corrmat['Resign'] #mengambil nilai korelasi Revenue
hasil = a[(a>0.05)|(a<-0.05)] #mengambil nilai korelasi yang lebih dari 0.1
hasil

df9_selection=df9[['Resign','SkorKepuasanPegawai','JumlahKeikutsertaanProjek','TingkatPendidikan','Pekerjaan_Data Analyst','Pekerjaan_DevOps Engineer','Pekerjaan_Digital Product Manager','Pekerjaan_Machine Learning Engineer','Pekerjaan_Product Design (UI & UX)','Pekerjaan_Scrum Master','Pekerjaan_Software Engineer (Back End)','Pekerjaan_Software Engineer (Front End)','AsalDaerah_Jakarta Barat','AsalDaerah_Jakarta Pusat','AsalDaerah_Jakarta Timur','HiringPlatform_Diversity_Job_Fair','HiringPlatform_Indeed','HiringPlatform_LinkedIn','HiringPlatform_Website','AlasanResign_Product Design (UI & UX)','AlasanResign_apresiasi','AlasanResign_ganti_karir','AlasanResign_internal_conflict','AlasanResign_jam_kerja','AlasanResign_kejelasan_karir','AlasanResign_leadership','AlasanResign_tidak_bahagia','AlasanResign_tidak_bisa_remote','AlasanResign_toxic_culture']]

df9_selection.info()

df9_selection['SkorKepuasanPegawai'] = df9_selection['SkorKepuasanPegawai'].astype('int')
df9_selection['JumlahKeikutsertaanProjek'] = df9_selection['JumlahKeikutsertaanProjek'].astype("int")

X = df9_selection[['SkorKepuasanPegawai','JumlahKeikutsertaanProjek','TingkatPendidikan','Pekerjaan_Data Analyst','Pekerjaan_DevOps Engineer','Pekerjaan_Digital Product Manager','Pekerjaan_Machine Learning Engineer','Pekerjaan_Product Design (UI & UX)','Pekerjaan_Scrum Master','Pekerjaan_Software Engineer (Back End)','Pekerjaan_Software Engineer (Front End)','AsalDaerah_Jakarta Barat','AsalDaerah_Jakarta Pusat','AsalDaerah_Jakarta Timur','HiringPlatform_Diversity_Job_Fair','HiringPlatform_Indeed','HiringPlatform_LinkedIn','HiringPlatform_Website','AlasanResign_Product Design (UI & UX)','AlasanResign_apresiasi','AlasanResign_ganti_karir','AlasanResign_internal_conflict','AlasanResign_jam_kerja','AlasanResign_kejelasan_karir','AlasanResign_leadership','AlasanResign_tidak_bahagia','AlasanResign_tidak_bisa_remote','AlasanResign_toxic_culture']]
y = df9_selection['Resign']

X, y

#Splitting the data into Train and Test
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

df9_selection['Resign'].value_counts()

from imblearn import under_sampling, over_sampling
X_under, y_under = under_sampling.RandomUnderSampler().fit_resample(X_train, y_train)
X_over, y_over = over_sampling.RandomOverSampler().fit_resample(X_train, y_train)
X_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE().fit_resample(X_train, y_train)

print('Original')
print(pd.Series(y).value_counts())
print('\n')
print('UNDERSAMPLING')
print(pd.Series(y_under).value_counts())
print('\n')
print('OVERSAMPLING')
print(pd.Series(y_over).value_counts())
print('\n')
print('SMOTE')
print(pd.Series(y_over_SMOTE).value_counts())

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def eval_classification(model):
    y_pred = model.predict(X_test)
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
    print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
    print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))
    print('AUC:'+ str(roc_auc_score(y_test, y_pred)))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

"""##Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=42)
lr.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(lr)

print('Train score: ' + str(lr.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score: ' + str(lr.score(X_test, y_test))) #accuracy

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# List Hyperparameters yang akan diuji
solver = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2','l1', 'elasticnet', 'none']
C = [100, 10, 1.0, 0.1, 0.01, 0.001, 0.0001]
hyperparameters = dict(penalty=penalty, C=C, solver=solver )

# Inisiasi model
logres = LogisticRegression(random_state=42) # Init Logres dengan Gridsearch, cross validation = 5
lr_tuned = RandomizedSearchCV(logres, hyperparameters, cv=5, random_state=42, scoring='recall')

# Fitting Model & Evaluation
lr_tuned.fit(X_over_SMOTE, y_over_SMOTE)
eval_classification(lr_tuned)

print('Train score: ' + str(lr.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score: ' + str(lr.score(X_test, y_test))) #accuracy

logres = LogisticRegression(penalty='l2', C=0.0001, solver='lbfgs', random_state=42)
logres.fit(X_over_SMOTE, y_over_SMOTE)
y_pred = logres.predict(X_test)

import math

feature_names = X_over_SMOTE.columns.to_list()

#Get the scores
score = logres.score(X_over_SMOTE.values, y_over_SMOTE)
print(score)
w0 = logres.intercept_[0]
w = logres.coef_[0]

feature_importance = pd.DataFrame(feature_names, columns = ['feature'])
feature_importance['importance'] = pow(math.e, w)
feature_importance = feature_importance.sort_values(by=['importance'],ascending=False)
feature_importance = feature_importance[:8].sort_values(by=['importance'], ascending=False)

#Visualization
ax = feature_importance.sort_values(by=['importance'], ascending=True).plot.barh(x='feature', y='importance')

plt.title('Important Feature')
plt.show()

print('Train score: ' + str(lr_tuned.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(lr_tuned.score(X_test, y_test))) #accuracy

"""##Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(dt)

print('Train score: ' + str(dt.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(dt.score(X_test, y_test))) #accuracy

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
import numpy as np

# List of hyperparameter
max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree
min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
max_features = ['auto', 'sqrt'] # Number of features to consider at every split

hyperparameters = dict(max_depth=max_depth, 
                       min_samples_split=min_samples_split, 
                       min_samples_leaf=min_samples_leaf,
                       max_features=max_features
                      )

# Inisialisasi Model
dt = DecisionTreeClassifier(random_state=42)
dt_tuned = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='precision')
dt_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(dt_tuned)

# plt.figsize(10, 8)
feat_importances = pd.Series(dt_tuned.best_estimator_.feature_importances_, index=X_over_SMOTE.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

"""##Random forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(rf)

print('Train score: ' + str(rf.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(rf.score(X_test, y_test))) #accuracy

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)], # Jumlah subtree 
                       bootstrap = [True], # Apakah pakai bootstrapping atau tidak
                       criterion = ['gini','entropy'],
                       max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],  # Maximum kedalaman tree
                       min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)], # Jumlah minimum samples pada node agar boleh di split menjadi leaf baru
                       min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)], # Jumlah minimum samples pada leaf agar boleh terbentuk leaf baru
                       max_features = ['auto', 'sqrt', 'log2'], # Jumlah feature yg dipertimbangkan pada masing-masing split
                       n_jobs = [-1], # Core untuk parallel computation. -1 untuk menggunakan semua core
                      )

# Init
rf = RandomForestClassifier(random_state=42)
rf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=42, scoring='recall')
rf_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(rf_tuned)

# plt.figsize(10, 8)
feat_importances = pd.Series(rf_tuned.best_estimator_.feature_importances_, index=X_over_SMOTE.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

"""##AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier
ab = AdaBoostClassifier(random_state=42)
ab.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(ab)

print('Train score: ' + str(ab.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(ab.score(X_test, y_test))) #accuracy

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

# List of hyperparameter
hyperparameters = dict(n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 2000)], # Jumlah iterasi
                       learning_rate = [float(x) for x in np.linspace(start = 0.001, stop = 0.1, num = 200)],  
                       algorithm = ['SAMME', 'SAMME.R']
                      )

# Init model
ab = AdaBoostClassifier(random_state=42)
ab_tuned = RandomizedSearchCV(ab, hyperparameters, random_state=42, cv=5, scoring='recall')
ab_tuned.fit(X_over_SMOTE, y_over_SMOTE)
print('Train score: ' + str(ab_tuned.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score: ' + str(ab_tuned.score(X_test, y_test))) #accuracy
# Predict & Evaluation
eval_classification(ab_tuned)

# plt.figsize(10, 8)
feat_importances = pd.Series(ab_tuned.best_estimator_.feature_importances_, index=X_over_SMOTE.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

"""##XGBoost"""

from xgboost import XGBClassifier
xg = XGBClassifier(random_state=42)
xg.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(xg)

print('Train score: ' + str(xg.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(xg.score(X_test, y_test))) #accuracy

show_feature_importance(xg)

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

#Menjadikan ke dalam bentuk dictionary
hyperparameters = {
                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]
                    }

# Init
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=5, random_state=42, scoring='recall')
xg_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(xg_tuned)

print('Train score: ' + str(xg_tuned.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(xg_tuned.score(X_test, y_test))) #accuracy

# plt.figsize(10, 8)
feat_importances = pd.Series(xg_tuned.best_estimator_.feature_importances_, index=X_over_SMOTE.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

"""##Best feature"""

X_imp = X_over_SMOTE[['JumlahKeikutsertaanProjek','TingkatPendidikan','HiringPlatform_LinkedIn','AsalDaerah_Jakarta Pusat','AsalDaerah_Jakarta Timur','HiringPlatform_Indeed','HiringPlatform_Diversity_Job_Fair','HiringPlatform_Website','AsalDaerah_Jakarta Barat','Pekerjaan_Software Engineer (Back End)']]
y_imp = y_over_SMOTE # target / label

#Splitting the data into Train and Test
from sklearn.model_selection import train_test_split 
X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_imp, y_imp, test_size = 0.3, random_state = 42)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def eval_class(model):
    y_pred_imp = model.predict(X_test_imp)
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test_imp, y_pred_imp))
    print("Precision (Test Set): %.2f" % precision_score(y_test_imp, y_pred_imp))
    print("Recall (Test Set): %.2f" % recall_score(y_test_imp, y_pred_imp))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test_imp, y_pred_imp))
    print('AUC:'+ str(roc_auc_score(y_test_imp, y_pred_imp)))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

from sklearn.ensemble import AdaBoostClassifier
ab_imp = AdaBoostClassifier(random_state=42)
ab_imp.fit(X_train_imp,y_train_imp)

eval_class(ab_imp)

from sklearn.metrics import confusion_matrix

#Generate the confusion matrix
y_pred_c = ab_imp.predict(X_test_imp)
cf_matrix = confusion_matrix(y_test_imp, y_pred_c)

print(cf_matrix)

group_names = ['True Negative','False Positive','False Negative','True Positive']
group_counts = ["{0:0.0f}".format(value) for value in
cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')
ax.set_title('Confusion Matrix\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])
## Display the visualization of the Confusion Matrix.
plt.show()

# Split Feature and Label
X_imp = X_over_SMOTE[['AlasanResign_jam_kerja','AlasanResign_ganti_karir','AlasanResign_kejelasan_karir','AlasanResign_toxic_culture','JumlahKeikutsertaanProjek','AlasanResign_tidak_bisa_remote','Pekerjaan_Software Engineer (Front End)','AlasanResign_tidak_bahagia']]
y_imp = y_over_SMOTE # target / label

#Splitting the data into Train and Test
from sklearn.model_selection import train_test_split 
X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_imp, y_imp, test_size = 0.3, random_state = 42)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def eval_class(model):
    y_pred_imp = model.predict(X_test_imp)
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test_imp, y_pred_imp))
    print("Precision (Test Set): %.2f" % precision_score(y_test_imp, y_pred_imp))
    print("Recall (Test Set): %.2f" % recall_score(y_test_imp, y_pred_imp))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test_imp, y_pred_imp))
    print('AUC:'+ str(roc_auc_score(y_test_imp, y_pred_imp)))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

from sklearn.linear_model import LogisticRegression
lr_imp = LogisticRegression(random_state=42)
lr_imp.fit(X_train_imp,y_train_imp)

eval_class(lr_imp)

from sklearn.metrics import confusion_matrix

#Generate the confusion matrix
y_pred_c = lr_imp.predict(X_test_imp)
cf_matrix = confusion_matrix(y_test_imp, y_pred_c)

print(cf_matrix)

group_names = ['True Negative','False Positive','False Negative','True Positive']
group_counts = ["{0:0.0f}".format(value) for value in
cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')
ax.set_title('Confusion Matrix\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])
## Display the visualization of the Confusion Matrix.
plt.show()

